# Natural-Language-Processing-with-Disaster-Tweets　- Kaggleコンペティション

このリポジトリは、Kaggleの「Natural-Language-Processing-with-Disaster-Tweets」分類コンペティションに取り組んだ内容をまとめたものです。
(https://www.kaggle.com/competitions/nlp-getting-started)

## 🏆 結果
- 最終順位：929チーム中 133位　上位15％（リーダーボード、2025年7月17日時点）
- 使用モデル：スタッキング（ベースモデル：LGBMClassifier + XGBoost　メタモデル：RandomForestClassifier）
- 評価指標：F1スコア
- 最終スコア：0.83481

## 📄 概要
ツイートが本当の災害についてのものか、そうでないかを予測する分類問題に取り組みました。

## 💻 開発環境
- Python 3.12.3
- Jupyter Notebook
- scikit-learn 1.6.1
- XGBoost 3.0.0
- lightgbm 4.6.0
- pandas 2.2.2
- numpy  1.26.4

## 🔍 アプローチ
- 特徴量エンジニアリング（特徴量の作成、テキストの処理）
- OOF予測、スタッキングモデル学習

## 📁 ファイル構成
- `NLP_tweets.ipynb`: 整理された最終モデル

## 📝 補足
本コンペティションを通じて、NLP分類問題における実践的なアプローチや特徴量エンジニアリング、スタッキングの手法について学ぶことができ、大変有意義な経験となりました。

特に、自然言語の特徴量の扱い方について理解を深めることができ、前処理の方法によってモデルの精度に大きな差が出ることを実感しました。
BERTとは何かなど新しく知識をつけなければならないことが多くとても大変ではありましたが、
自然言語処理における多様な手法の存在を知ることができ、今後さらに理解を深めていきたいと感じています。

今回は英語テキストを対象としましたが、日本語を扱う場合は処理の複雑さが増すことが予想されます。
今後は、より高度なテキスト処理技術を身につけていきたいと考えています。

